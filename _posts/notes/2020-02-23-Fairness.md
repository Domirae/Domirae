Fairness in Machine Learning

## Introduction & Motivation
최근 머신러닝 분야에서 'Fairness' 분야는 인기있는 분야 중 하나가 되었습니다. 그렇다면 왜 우리가 'Fairness'에 대해서 생각해야하는 것일까요?

요즘 많은 요소들이 머신러닝에 의해 자동화되고 있는 시대에 살고 있습니다. 하지만 여전히 정확하지 않다는 한계를 가지고 있습니다. 왜냐하면 머신러닝은 'relies heavily on data' 즉, 데이터에 의존적이라는 것입니다. 인공지능은 인간이 가르치는 것을 배우는 것에만 객관적이라고 할 수 있습니다. 

예를들어 재범확률을 예측한 COMPAS알고리즘은 백인에 비해 흑인이 훨씬 더 높은 false positive rate를 산출하는 것을 볼 수 있었습니다. 

[image:958CAA1A-11FF-451E-B6E9-C3490306B318-8235-000005ED580E8B38/image.png]

링크드인과 비슷한 플랫폼인 XING에서도, 성별을 제외한 다른 종목에서 우월한 점수를 가지고 있는 여성 후보보다, 낮은 점수를 가진 남성 후보자를 더 높게 순위를 매기는 것으로 밝혀졌습니다. 

[image:A5DA7D87-13E0-4790-A7D4-FE27EDB3BA87-8235-000005EE629A2837/image.png]
 
머신러닝에서 Bias라는 것은 application이 '사람'과 관련되어있을 때 거의 어디서나 볼 수 있었습니다. 그리고 이는 소수그룹 혹은 역사적으로 불리했던 그룹의 이익을 해치는 요소라고 볼 수 있습니다. (위의 사례에서 볼 수 있음.)

## Causes 
그렇다면 머신러닝 시스템에서 bias가 생기는 원인은 무엇일까요? 이유 중 하나는 역사적인 이유 때문에 학습 데이터셋에 '사람의 편견'이 들어있는 것입니다. 

* 왜곡된 샘플 : 예를들어 범죄 기록은 경찰이 관찰한 범죄에서 나온 것으로, 애초에 범죄율이 더 높은 것에 경찰을 더 많이 파견하는 경향이 있을 것이다. 따라서 이런 지역에서 범죄를 기록할 가능성이 더 높다. 다른 지역의 사람들이 나중에 더 높은 범죄율을 갖게 되어도, 경찰의 파견의 정도가 다르기 때문에 여전히 다른 지역은 더 낮다고 표현될 수 있다. 이런식으로 수집된 데이터를 사용하여 학습된 시스템은 경찰이 적은 지역에 대해서 긍적적인 편향 (편견)을 갖는 경향이 있을 것이다.
* Tainted sample : 어떤 시스템이 지원자를 선택하기 위해, 이전에 인사관리팀이 내린 채용 결정을 라벨로 사용하여 학습한 경우, 학습된 시스템은 인사관리팀의 결정에 존재하는 bias를 학습할 것이다. 또 다른 예시로 구글뉴스를 이용하여 학습된 단어 임베딩 같은 경우, '프로그래머'는 '남성'과 'homemaker'같은 경우 '여성'과 관계가 매우 유사한 것으로 밝혀졌다. 
* Limited features : 데이터의 feature가 소수집단에 대해 비신뢰적으로 수집될 수 있다. 이는 소수집단의 데이터 라벨의 신뢰성이 다수 집단의 데이터보다 낮은 경우, 시스템은 이러한 노이즈 값으로 인해 소수집단의 예측 정확도가 훨씬 낮은 것으로 밝혀졌다.
* 표본 크기 불일치 : 소수 집단에서 나오는 데이터가 다수 집단의 학습 데이터보다 훨씬 적다면, 소수집단을 완벽하게 모델링할 가능성이 낮다.
*  Proxies : 인종과 성별과 같이 민감한 속성을 머신러닝 학습에 사용하지 않더라도,  이런 민감한 속성의 대용물이 될 수 있는 다른 특징이 항상 존재할 수 있다는 것이다. 따라서 이런 특징들이 포함된다면, bias는 여전히 일어날 것이다. 주로 이러한 특성이 'protected features (인종과 성별과 같은 feature 들)'과 관련이 있는지, 그리고 학습에 포함해야하는지 판단하기 어렵다. 


다음과 같은 문제를 세가지 그룹으로 분류할 수 있다. 

(1) performance에서 관찰되지 않은 차이 : skewed sample, tainted examples
(2) performance에서 관찰된 Sample Coping : limited features, sample size disparity
(3) Understanding the causes of disparities in predicted outcome : proxies

## Fairness의 정의 
'Fairness'를 어떻게 정의할 수 있는가?, 머신러닝 시스템에서 고려할 수 있도록 Fairness를 어떻게 공식화할 수 있는가

대부분은 다음 여섯개의 요소를 기반으로 한다.

* Unawareness 
* Demographic Parity
* Equalized Odds 
* Predictive Rate Parity
* Individual Fairness 
* Counterfactual fairness

이 중 Demo grahpic parity, Equalized Odds, Predictive Rate Parity는 'Group Fairness'에 속한다고 한다.

## Setup and Notation
하나의 sensitive attribute를 가지고 있는 이진 분류 문제를 이용해 위의 요소를 설명한다. 



* X ∈ Rᵈ : application의 정량화된 feature (ex. 교육, 업무 경험..)
* A ∈ {0, 1} : 이진 sensitive attribute (ex. Majority / minority)
* C :=c(X,A) ∈ {0,1} : R:=r(x, a) ∈ [0,1]  값을 기반으로 결정을 내리는 binary predictor (ex.hire/reject)
* Y ∈ {0, 1} : 타겟 값
* (X,A,Y) ~ D : X,A,Y는 기본 분포 D에서 생성된다고 가정함.
* P₀ [c] := P [c | s = 0] 로 나타낼 수 있음. 

초기, Fainess 제약을 걸지 않을때는 정확성을 올리는 것을 최적화합니다. 그리고 C(X, S)=Y ∀ (X, S, Y) ~ D일때 최고의 정확도를 달성합니다. 

##### Data 

이를 설명하기 위해서 간단한 예시를 들어서 설명하겠습니다. 
단순히 '소득'에 근거하여 신용 모델을 만들었다고 가정합니다. 이 모델은 대출금을 모두 채납한 사람과 그렇지 않은 사람들의 소득을 학습하여 구별하는 것을 목표로 합니다. 즉 미래에 어떤 사람이 대출을 받을 것인지 결정하기 위해서 학습 데이터셋에서 소득 임계값을 설정하는 것입니다. ( 아래 이미지에서 회색선이 소득 임계값입니다.) 

[image:4B5C0A76-8EE9-45EC-BD75-92E3E4931A35-8235-000006216E1CAFC8/image.png]

* Paid loan: 대출금을 다 청산한 사람
* Defaulted: 대출금을 갚지 못하고 체납된 사람

앞서 설명한 것과 같이 Demographic Parity는 성별과 같이 보호받는 속성에 대해서 동등한 비율로 긍정적인 결과를 받아야한다는 것을 전제로 합니다. '긍정적인 결과'는 "대출받기", "광고 보여지기", "대학합격"과 같이 선호되는 결정을 의미합니다. 앞에서 말한 것처럼 그 차이가 이상적으로 0이어야만 하지만 현실적으로는 그렇지 않습니다. 



 [image:B89BA377-7E4B-4896-A90B-4327F8781A6F-8235-00000683080D3228/1*xhAA1iHxG4UCzY_5RanIjg.png]
<figure 2 : use of different income thresholds to achieve Demographic Parity>



### Unawareness 

간단히 sensitive attribute를 학습 데이터의 feature에서 제외하는 방법이다. 

*Formulation* : C=c(x, A) = c(X)

*단점* : sensitive attribute만 제거한다고 해서 이 속성들과 관련있는 다른 상관관계들이 사라지는 것이 아니기 때문에 한계가 존재한다.

### Demographic Parity

Independence, Statistical Parity라고도 불리우는 Demographic Parity는 fairness 중 잘 알려진 방식 중 하나이다. 간단하게 설명하자면 이상적인 세계에서는 긍정적인 결과를 얻는 그룹들의 차이는 '0'이어야 하는 것이다. 하지만 현실에서는 이런일이 거의 일어나지 않기 때문에 이런 '차이'를 추적하는 방식이다. 

예를들어 figure 2에서 순수하게 "소득"을 근거로 하여 신용 모델을 만들었다고 가정했을 때, Group A 와 B에서 각자 대출을 받는 사람들의 "비율"이 같도록 각 Group에 대해서 서로 다른 "requirement levels'을 사용하도록 결정할 수 있다는 것입니다. 


#### Mathematical definition
수학적 관점에서 "Demographic Parity"는 모델의 outcome이protected class인  A에 대해서 독립적일 것을 요구합니다. 
[image:6A118ED9-4A64-4C58-A66F-E2680F8C4F71-10411-000007D9C3B3AB87/1*z4mVq1cai7vj54PbmdOZ2A.png]


C is independent of A: P₀ [C = c] = P₁ [C = c] ∀ c ∈ {0,1}

이는 두개의 그룹의 지원자 합격비율이 동일해야함을 의미합니다. Group1을 advantageous 그룹이라고 가정했을 때, fairness가 고려되지 않을 때는 높은 고용 비율을 가지고 있습니다.

##### Problem

댄스 그룹을 뽑을 때 프로댄서, 아이돌 그리고 소수의 배우를 선택한다고 하자. 이전 고정관념은 프로댄서나 아이돌이 더 적합하다고 말하고, 배우들은 그렇지 않을 것이라고 말할 것이다. 하지만 배우는 프로댄서나 아이돌과 같이 춤을 배울 수 있는 기회가 많이 없었다는 것을 것이다. 따라서 demographic parity를 고려한 모델로 의사결정을 내릴 때, 여전히 우리는 고정관념에 갖혀있을 수 있는 것이다. 

##### When use Demigraphic Parity

* minority group을 조금 더 많이 보고 싶을 때
* 역사적 고정관념이 우리의 모델에 영향을 주기를 바랄 때 
* minority group을 서포트할 수 있는 plan이 있어서 역사적 고정관념을 막을 수 있을 때.



### Equal Oppiotunity 

![image](https://user-images.githubusercontent.com/26568793/77726388-1e2f0200-703b-11ea-9707-c468941f4867.png)

각각의 그룹이 같은 비율의 positive outcome을 가지며, 이 그룹의 사람들이 이에 대한 자격을 갖추고 있어야 한다. (Classifier is indenpendent when target & sensitive attr is ...) Recall rate (True positive rate)을 동일하게 하는 것으로 

"너가 춤을 출 수 있다면, 댄스 그룹의 일부가 될 수 있다"

##### Mathematical definition

positive outcome이 protected class인 A와 조건(결과값) Y가 실제 positive인 것을 가정하에 독립인 것을 가정으로 한다. 

![image](https://user-images.githubusercontent.com/26568793/77726728-ebd1d480-703b-11ea-8858-dd383ad1183b.png)

confusion matrix를 기반으로 TPR이 protected class에 대해서 동일해야한다는 것인데
실제 TPR 비율의 차이가 0과 같도록 하기 어렵기 때문에 해당 격차를 최소화하는 것을 목표로한다. 

##### Problem

![image](https://user-images.githubusercontent.com/26568793/77726926-7b778300-703c-11ea-901b-33f1d271f76f.png)
그림에서 볼 수 있듯이, Equal odds가 요구하는 대로 TPR이 서로 같은 것을 볼 수 있다. 
하지만 Group B를 보면 상당히 많은 FP가 도입된 것을 볼 수 있다. (9/18) 실제로는 춤을 출 수 없는데 출 수 있다고 분류된 경우로 이는 minor group B 모델의 신용도가 떨어지는 것을 의미한다.

##### 언제 사용할까?
* positive outcome을 정확하게 예측하는데 중점을 두고 있을 때
* False Positive Data에 크게 비용을 두지 않을 때
* target 변수가 주관적으로 평가되는 것이 아닐 때. 


### Equalised odds 
![image](https://user-images.githubusercontent.com/26568793/77729429-fbecb280-7041-11ea-8536-34b2f309fd67.png)

가장 restrictive한 개념으로 
* Equal opportunity와 동일하게 positvie outcome을 그룹간에 동일한 비율로 인지하게하고
* group간에 동일한 비율의 positive outcome 중 miss-classify된 데이터를 모두 고려한다. (FP 비율 역시 동일하게 만든다.)

위 그림을 통해 TPR과 FPR을 모두 동일한 비율로 설정한 것을 볼 수 있다.

##### Mathematical definition

![image](https://user-images.githubusercontent.com/26568793/77729869-03f92200-7043-11ea-9b0d-ad2ab58d1d95.png)

True Positive Rate (A=0) = True Positive Rate (A=1) , and
False Positive Rate (A=0) = False Positive Rate (A=1)

##### Problem

![image](https://user-images.githubusercontent.com/26568793/77729915-1e330000-7043-11ea-903a-59ac391a5ce3.png)

major group에서 정확도를 최적화하지 못해 정확도가 떨어질 수 있다. 
다음 표를 통해서 profit이 odd만 가장 낮은 것을 볼 수 있다. 

모델이 구축되기 전에 bias를 감지하거나 완화하기 위한 작업이 수행되지 않았으며, 이로인해 profit이 낮은 이유를 설명할 수 없다. 
이를 통해서 어떤 bias mitigation방법을 사용하는 것이 중요한지 보여주고있다. 

##### 언제 사용하는가

* positive outcome이 정말 중요한 경우 
* False positive 데이터를 줄이는 것이 정말 중요할 때
* target 정보가 주관적인 것이 아닐때
* 해당 ceriterion을 통해 모델의 reward function이 저하되는 것이 아닐 때.

---
#### 참고링크

다음 사이트를 참고 및 번역한 게시글입니다.
 
 https://towardsdatascience.com/how-to-define-fairness-to-detect-and-prevent-discriminatory-outcomes-in-machine-learning-ef23fd408ef2

https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb


