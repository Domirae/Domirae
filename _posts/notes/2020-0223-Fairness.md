Fairness in Machine Learning

## Introduction & Motivation
최근 머신러닝 분야에서 'Fairness' 분야는 인기있는 분야 중 하나가 되었습니다. 그렇다면 왜 우리가 'Fairness'에 대해서 생각해야하는 것일까요?

요즘 많은 요소들이 머신러닝에 의해 자동화되고 있는 시대에 살고 있습니다. 하지만 여전히 정확하지 않다는 한계를 가지고 있습니다. 왜냐하면 머신러닝은 'relies heavily on data' 즉, 데이터에 의존적이라는 것입니다. 인공지능은 인간이 가르치는 것을 배우는 것에만 객관적이라고 할 수 있습니다. 

예를들어 재범확률을 예측한 COMPAS알고리즘은 백인에 비해 흑인이 훨씬 더 높은 false positive rate를 산출하는 것을 볼 수 있었습니다. 

[image:958CAA1A-11FF-451E-B6E9-C3490306B318-8235-000005ED580E8B38/image.png]

링크드인과 비슷한 플랫폼인 XING에서도, 성별을 제외한 다른 종목에서 우월한 점수를 가지고 있는 여성 후보보다, 낮은 점수를 가진 남성 후보자를 더 높게 순위를 매기는 것으로 밝혀졌습니다. 

[image:A5DA7D87-13E0-4790-A7D4-FE27EDB3BA87-8235-000005EE629A2837/image.png]
 
머신러닝에서 Bias라는 것은 application이 '사람'과 관련되어있을 때 거의 어디서나 볼 수 있었습니다. 그리고 이는 소수그룹 혹은 역사적으로 불리했던 그룹의 이익을 해치는 요소라고 볼 수 있습니다. (위의 사례에서 볼 수 있음.)

## Causes 
그렇다면 머신러닝 시스템에서 bias가 생기는 원인은 무엇일까요? 이유 중 하나는 역사적인 이유 때문에 학습 데이터셋에 '사람의 편견'이 들어있는 것입니다. 

* 왜곡된 샘플 : 예를들어 범죄 기록은 경찰이 관찰한 범죄에서 나온 것으로, 애초에 범죄율이 더 높은 것에 경찰을 더 많이 파견하는 경향이 있을 것이다. 따라서 이런 지역에서 범죄를 기록할 가능성이 더 높다. 다른 지역의 사람들이 나중에 더 높은 범죄율을 갖게 되어도, 경찰의 파견의 정도가 다르기 때문에 여전히 다른 지역은 더 낮다고 표현될 수 있다. 이런식으로 수집된 데이터를 사용하여 학습된 시스템은 경찰이 적은 지역에 대해서 긍적적인 편향 (편견)을 갖는 경향이 있을 것이다.
* Tainted sample : 어떤 시스템이 지원자를 선택하기 위해, 이전에 인사관리팀이 내린 채용 결정을 라벨로 사용하여 학습한 경우, 학습된 시스템은 인사관리팀의 결정에 존재하는 bias를 학습할 것이다. 또 다른 예시로 구글뉴스를 이용하여 학습된 단어 임베딩 같은 경우, '프로그래머'는 '남성'과 'homemaker'같은 경우 '여성'과 관계가 매우 유사한 것으로 밝혀졌다. 
* Limited features : 데이터의 feature가 소수집단에 대해 비신뢰적으로 수집될 수 있다. 이는 소수집단의 데이터 라벨의 신뢰성이 다수 집단의 데이터보다 낮은 경우, 시스템은 이러한 노이즈 값으로 인해 소수집단의 예측 정확도가 훨씬 낮은 것으로 밝혀졌다.
* 표본 크기 불일치 : 소수 집단에서 나오는 데이터가 다수 집단의 학습 데이터보다 훨씬 적다면, 소수집단을 완벽하게 모델링할 가능성이 낮다.
*  Proxies : 인종과 성별과 같이 민감한 속성을 머신러닝 학습에 사용하지 않더라도,  이런 민감한 속성의 대용물이 될 수 있는 다른 특징이 항상 존재할 수 있다는 것이다. 따라서 이런 특징들이 포함된다면, bias는 여전히 일어날 것이다. 주로 이러한 특성이 'protected features (인종과 성별과 같은 feature 들)'과 관련이 있는지, 그리고 학습에 포함해야하는지 판단하기 어렵다. 


다음과 같은 문제를 세가지 그룹으로 분류할 수 있다. 

(1) performance에서 관찰되지 않은 차이 : skewed sample, tainted examples
(2) performance에서 관찰된 Sample Coping : limited features, sample size disparity
(3) Understanding the causes of disparities in predicted outcome : proxies

## Fairness의 정의 
'Fairness'를 어떻게 정의할 수 있는가?, 머신러닝 시스템에서 고려할 수 있도록 Fairness를 어떻게 공식화할 수 있는가

대부분은 다음 여섯개의 요소를 기반으로 한다.

* Unawareness 
* Demographic Parity
* Equalized Odds 
* Predictive Rate Parity
* Individual Fairness 
* Counterfactual fairness

이 중 Demo grahpic parity, Equalized Odds, Predictive Rate Parity는 'Group Fairness'에 속한다고 한다.

## Setup and Notation
하나의 sensitive attribute를 가지고 있는 이진 분류 문제를 이용해 위의 요소를 설명한다. 



* X ∈ Rᵈ : application의 정량화된 feature (ex. 교육, 업무 경험..)
* A ∈ {0, 1} : 이진 sensitive attribute (ex. Majority / minority)
* C :=c(X,A) ∈ {0,1} : R:=r(x, a) ∈ [0,1]  값을 기반으로 결정을 내리는 binary predictor (ex.hire/reject)
* Y ∈ {0, 1} : 타겟 값
* (X,A,Y) ~ D : X,A,Y는 기본 분포 D에서 생성된다고 가정함.
* P₀ [c] := P [c | s = 0] 로 나타낼 수 있음. 

초기, Fainess 제약을 걸지 않을때는 정확성을 올리는 것을 최적화합니다. 그리고 C(X, S)=Y ∀ (X, S, Y) ~ D일때 최고의 정확도를 달성합니다. 

##### Data 

이를 설명하기 위해서 간단한 예시를 들어서 설명하겠습니다. 
단순히 '소득'에 근거하여 신용 모델을 만들었다고 가정합니다. 이 모델은 대출금을 모두 채납한 사람과 그렇지 않은 사람들의 소득을 학습하여 구별하는 것을 목표로 합니다. 즉 미래에 어떤 사람이 대출을 받을 것인지 결정하기 위해서 학습 데이터셋에서 소득 임계값을 설정하는 것입니다. ( 아래 이미지에서 회색선이 소득 임계값입니다.) 

[image:4B5C0A76-8EE9-45EC-BD75-92E3E4931A35-8235-000006216E1CAFC8/image.png]

* Paid loan: 대출금을 다 청산한 사람
* Defaulted: 대출금을 갚지 못하고 체납된 사람

앞서 설명한 것과 같이 Demographic Parity는 성별과 같이 보호받는 속성에 대해서 동등한 비율로 긍정적인 결과를 받아야한다는 것을 전제로 합니다. '긍정적인 결과'는 "대출받기", "광고 보여지기", "대학합격"과 같이 선호되는 결정을 의미합니다. 앞에서 말한 것처럼 그 차이가 이상적으로 0이어야만 하지만 현실적으로는 그렇지 않습니다. 



 [image:B89BA377-7E4B-4896-A90B-4327F8781A6F-8235-00000683080D3228/1*xhAA1iHxG4UCzY_5RanIjg.png]
<figure 2 : use of different income thresholds to achieve Demographic Parity>



### Unawareness 

간단히 sensitive attribute를 학습 데이터의 feature에서 제외하는 방법이다. 

*Formulation* : C=c(x, A) = c(X)

*단점* : sensitive attribute만 제거한다고 해서 이 속성들과 관련있는 다른 상관관계들이 사라지는 것이 아니기 때문에 한계가 존재한다.

### Demographic Parity

Independence, Statistical Parity라고도 불리우는 Demographic Parity는 fairness 중 잘 알려진 방식 중 하나이다. 간단하게 설명하자면 이상적인 세계에서는 긍정적인 결과를 얻는 그룹들의 차이는 '0'이어야 하는 것이다. 하지만 현실에서는 이런일이 거의 일어나지 않기 때문에 이런 '차이'를 추적하는 방식이다. 

예를들어 figure 2에서 순수하게 "소득"을 근거로 하여 신용 모델을 만들었다고 가정했을 때, Group A 와 B에서 각자 대출을 받는 사람들의 "비율"이 같도록 각 Group에 대해서 서로 다른 "requirement levels'을 사용하도록 결정할 수 있다는 것입니다. 


#### Mathematical definition
수학적 관점에서 "Demographic Parity"는 모델의 outcome이protected class인  A에 대해서 독립적일 것을 요구합니다. 
[image:6A118ED9-4A64-4C58-A66F-E2680F8C4F71-10411-000007D9C3B3AB87/1*z4mVq1cai7vj54PbmdOZ2A.png]


C is independent of A: P₀ [C = c] = P₁ [C = c] ∀ c ∈ {0,1}

이는 두개의 그룹의 지원자 합격비율이 동일해야함을 의미합니다. Group1을 advantageous 그룹이라고 가정했을 때, fairness가 고려되지 않을 때는 높은 고용 비율을 가지고 있습니다.   




---
#### 참고링크

다음 사이트를 참고 및 번역한 게시글입니다.
 
 https://towardsdatascience.com/how-to-define-fairness-to-detect-and-prevent-discriminatory-outcomes-in-machine-learning-ef23fd408ef2

https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb


